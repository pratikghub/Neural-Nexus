---
title: |
    | Neural Network Hyperparameter Tuning for Flight Delay Classification
author: "Pratik Ganguli"
format: pdf
---



# Title: Classification Analysis of Late Flight Arrivals

This example is adapted from <https://www.tidymodels.org/start/recipes/>


Load required packages
```{r, eval = TRUE}
library(tidymodels)
library(themis)
library(nycflights13)
#install.packages("nnet")
#install.packages("NeuralNetTools")
library(nnet)
library(NeuralNetTools)
              
tidymodels_prefer()

```

Question of interest: 
Will a plane departing from a New York City airport arrive more than 30 minutes late?

Data: all flights from New York City airports in 2013

## Data Preparation 

## Data Preparation: Data Transformation 1
```{r}
#| label: data_prep

set.seed(123)
flight_data <- 
  flights |> 
  mutate(
    # Convert the arrival delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    # We will use the date (not date-time) in the recipe below
    date = lubridate::as_date(time_hour)
  ) |> 
  # Include the weather data at origin
  inner_join(weather, by = c("origin", "time_hour")) |> 
  # Take a random sample of flights (original dataset is too big)
  # Only do this for demo purposes. 
  # On real data use the whole dataset
  slice_sample(n = 10000)

```
##  Data Transformation 2
```{r}
#| label: data_selection

flight_data <- flight_data |> 
  # Only retain the specific columns we will use
  select(flight, time_hour, arr_delay, 
         date,  dep_time,
         #air_time,  distance, 
         #temp, carrier, origin, dest,
         )  |>  
  # Exclude flights with missing arrival delay
  filter(!is.na(arr_delay)) |>   
  # Encode qualitative columns as factors (instead of character strings)
  mutate_if(is.character, as.factor)
```

Note:
We won't use variables `flight` and `time_hour` in the model, but keeping them in the dataset is useful for identification. 
 
## Modelling

## Modelling: Pre-processing: Partition the data

80% training, 20% test

```{r}
#| label: data_split

# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
# when random numbers are used 
set.seed(222)

# Split 80% of the data into the training set 
data_split <- initial_split(flight_data, prop = 0.8, strata = arr_delay)

# Create data frames for the three sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

## Modelling: Pre-processing: Recipe

```{r}
flights_rec <- 
  # create recipe and specify formula
  recipe(arr_delay ~ ., data = train_data)  |>  
  # update role of ID variables  
  update_role(flight, time_hour, new_role = "ID") |> 
  # pre-process dates - extract day of week and month
  step_date(date, features = c("dow", "month"), 
            keep_original_cols = FALSE) |>               
  # normalize variables (required for knn)
  step_normalize(all_numeric_predictors()) |>   
  # create dummy variables for nominal predictors
  step_dummy(all_nominal_predictors())|> 
  # remove zero variance predictors
  step_zv(all_predictors())  |> 
  # use upsampling to address class imbalance
  step_upsample(arr_delay, over_ratio = 1)
```

**Inspect the impact of the recipe**

```{r}
flights_prepped <- 
flights_rec |> 
  prep() |> 
  bake(new_data = NULL) 

flights_prepped |> glimpse()
flights_prepped |>  count(arr_delay)
```


## Tuning: Specify model and workflow

Does the number of hidden nodes and number of iterations make a difference?
`tidymodels` has some functions facilitate trying (tuning) different parameters.


Notes:

* The `mlp` has 1 hidden layer, so `hidden_units` specifies the number nodes in the hidden layer.
* epochs are the number of "passes" through the training data



**Specify model**

```{r}
nn_model_tune <-
  mlp(hidden_units = tune(),  # number of nodes in hidden layer
      epochs = tune() # number of iterations
      ) |> 
  set_engine("nnet") |> 
  set_mode("classification")
```

**Create workflow = recipe + model**

```{r, results = "hide"}
nn_tune_wkflow <- workflow() |> 
                  add_model(nn_model_tune) |> 
                  add_recipe(flights_rec)
```

## Tuning: Define values to test

**Create set of values to test**

```{r}
# grid_regular chooses sensible values for the parameters
nn_grid <- grid_regular(hidden_units(),
                            epochs(),
                          levels = c(5, 3))
print(nn_grid, n = 16)

```

## Tuning: Use cross validation to compare hyperparameter values

**Create 5 cross-validation folds using training data**

```{r}
set.seed(747)
flights_folds <- vfold_cv(train_data, v = 5, strata = arr_delay)
flights_folds
```
 

**Fit model on each fold**
```{r}
flights_metric <- metric_set(
  accuracy, 
  roc_auc,
  sens, spec, bal_accuracy)

Sys.time()

flights_res <- nn_tune_wkflow |> 
  tune_grid(
    # select object containing folds
    resamples = flights_folds,
    # specify grid of values to evaluate
    grid = nn_grid,
    # specify set of metric (optional)
    metrics = flights_metric,
    # save predictions
    control = control_grid(save_pred = TRUE)
    )
Sys.time()

```


**Inspect results**
```{r}
flights_res

# inspect metrics for first fold
flights_res$.metrics[[1]]
```


## Tuning: Evaluate performance of each $k$  

**Inspect results for each fold**

```{r}
flights_res |> collect_metrics(summarize = FALSE) |> print(n=7)
```



**Inspect summarised results**
```{r}
flights_res |> collect_metrics() |> print(n=7)
```

**Collect predictions**

```{r}
flights_pred <- flights_res |>  collect_predictions()
flights_pred
```



**Plot ROC curves**

```{r}
flights_pred |>
  group_by(id, hidden_units, epochs) |># id contains folds
  roc_curve(truth = arr_delay, .pred_late) |>
  autoplot() +
  facet_grid(rows = vars(hidden_units), cols = vars(epochs))+ theme(legend.position = "none")
```

**Plot metrics**

```{r}
g <-
flights_res |>
  collect_metrics() |>
  mutate(hidden_units = factor(hidden_units),
         epochs = factor(epochs)) |>
  ggplot(aes(hidden_units, mean, color = epochs, group = epochs)) +
  geom_line(linewidth = 1.5) +
  geom_point(size = 2) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)+
  facet_grid(rows =  vars(.metric))
g 
```

Alternative representation
```{r}
g <-
flights_res |>
  collect_metrics() |>
  mutate(hidden_units = factor(hidden_units),
         epochs = factor(epochs)) |>
  ggplot(aes(x = .metric, y=mean, fill = .metric, group = .metric)) +
  geom_col(linewidth = 1.5) +
  #geom_point(size = 2) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)+
  facet_grid(rows =  vars(hidden_units ), cols = vars(epochs))+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
g 
```



## Tuning: Select the best model

In some applications, it is clear which metric should be used. 
In this case, use `select_best` with the specified metric.

However sometimes, it is worth making a trade-off to sacrifice 
the score on one metric, to avoid very poor score on another metric,
or to reduce computational complexity (e.g. run-time or number of parameters).
These types of trade-offs usually have to be performed manually.
The following code demonstrates how to inspect the metrics.


```{r}
flights_res |> select_best(metric ="roc_auc")
flights_res |> select_best(metric ="sens")
```


Inspect training performance for selected parameters
```{r}
flights_res |>
  collect_metrics() |> filter(.config %in% c("Preprocessor1_Model02", 
                                             "Preprocessor1_Model11", 
                                             "Preprocessor1_Model12"))
```

Select the best model
```{r}

(selected_nn_model <- 
  flights_res |>
   select_best(metric ="roc_auc")
   #select_best(metric ="sens")
   )

## alternatively, manually specify selected model
# selected_nn_model <- tibble(
#   hidden_units = 5,
#   epochs = 10,
#   rowNumber = 2, #obtain from nn_grid
#   .config = paste0("Preprocessor1_Model", ifelse(rowNumber<10, "0", ""), rowNumber)
# )

flights_res |> collect_metrics()|> filter(.config == selected_nn_model$.config)
```



## Tuning:  Finalise the workflow
```{r}
final_nn_tune_wkflow <-
  nn_tune_wkflow |>
  finalize_workflow(selected_nn_model)
```

## Tuning:  Final fit


Do a final fit (train on all training data and test on testing data)

```{r}
final_fit <- 
  final_nn_tune_wkflow |>
  last_fit(data_split,
               metrics = flights_metric)
```

Note: choice of metrics will vary for different applications.

## Tuning:  Evaluate final fit

```{r}
final_fit |>
  collect_metrics()
```

## Tuning:  Collect predictions and plot ROC curve

```{r}
final_fit |>
  collect_predictions() |> 
  roc_curve(truth = arr_delay, .pred_late) |> 
  autoplot()
```
 